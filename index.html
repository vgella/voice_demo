<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Voice Agent Demo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      color-scheme: light dark;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
    }
    body {
      margin: 0;
      padding: 24px;
      background: #0f172a;
      color: #e2e8f0;
    }
    h1 { margin-top: 0; font-size: 1.6rem; }
    .panel {
      background: rgba(15, 23, 42, 0.85);
      border: 1px solid rgba(148, 163, 184, 0.3);
      border-radius: 12px;
      padding: 20px;
      max-width: 900px;
      margin: 0 auto 20px auto;
      box-shadow: 0 18px 45px rgba(15, 23, 42, 0.45);
    }
    label { display:block; margin-bottom:4px; font-weight:500; }
    input[type="text"]{
      width: 100%;
      padding: 10px 12px;
      border-radius: 8px;
      border: 1px solid rgba(148, 163, 184, 0.35);
      background: rgba(15, 23, 42, 0.6);
      color: inherit;
      font-size: 1rem;
      margin-bottom: 12px;
    }
    .row { display:flex; gap:12px; flex-wrap: wrap; align-items:center; }
    .control {
      display:flex; align-items:center; gap:8px;
      padding: 8px 10px;
      border-radius: 10px;
      border: 1px solid rgba(148, 163, 184, 0.25);
      background: rgba(15, 23, 42, 0.45);
    }
    .control label { margin:0; font-size: .9rem; opacity:.9; }
    .control input[type="range"]{ width: 180px; }
    button {
      cursor: pointer;
      border: none;
      border-radius: 24px;
      padding: 10px 18px;
      margin-right: 10px;
      margin-bottom: 10px;
      font-size: 0.95rem;
      transition: transform 0.15s ease, opacity 0.2s ease;
      background: linear-gradient(135deg, #38bdf8, #0ea5e9);
      color: #0f172a;
      font-weight: 600;
    }
    button.secondary {
      background: transparent;
      color: #e2e8f0;
      border: 1px solid rgba(148, 163, 184, 0.4);
    }
    button:disabled { opacity: 0.45; cursor: not-allowed; }
    button:not(:disabled):hover { transform: translateY(-1px); }

    #log { max-height: 420px; overflow-y: auto; padding-right: 8px; }
    .message {
      margin-bottom: 14px;
      padding: 12px 14px;
      border-radius: 10px;
      line-height: 1.4;
      background: rgba(30, 41, 59, 0.7);
    }
    .message.user { border-left: 3px solid #38bdf8; }
    .message.agent { border-left: 3px solid #a855f7; }
    #status { font-size: 0.9rem; margin-top: 12px; opacity: 0.85; }
    .small-note { font-size: 0.8rem; opacity: 0.7; }
    .warn { color: #fbbf24; }
    @media (max-width: 600px) {
      body { padding: 16px; }
      .panel { padding: 16px; }
      .control input[type="range"]{ width: 140px; }
    }
  </style>
</head>

<body>
  <div class="panel">
    <h1>Voice Agent Browser Demo</h1>
    <div id="sessionInfo" class="small-note"></div>
    <p>
      This page continuously listens (optional) and uses thresholding to detect speech start/end.
      It sends audio to the backend for transcription.
      It also supports barge-in (interrupt playback when you start talking).
    </p>

    <label for="serverUrl">Voice agent URL</label>
    <input id="serverUrl" type="text" autocomplete="off" />

    <div class="row">
      <button id="listenBtn">ðŸŽ¤ Start Listening</button>
      <button id="stopBtn" class="secondary">Stop Listening</button>
      <button id="sendBtn" class="secondary">Send Typed Message</button>
    </div>

    <div class="row" style="margin: 6px 0 12px 0;">
      <div class="control">
        <label><input id="alwaysOnToggle" type="checkbox" checked /> Always listening (VAD)</label>
      </div>
      <div class="control">
        <label for="vadThreshold">Threshold</label>
        <input id="vadThreshold" type="range" min="0.01" max="0.25" step="0.005" value="0.06" />
        <span id="vadThresholdVal" class="small-note">0.06</span>
      </div>
      <div class="control">
        <label for="silenceMs">Silence ms</label>
        <input id="silenceMs" type="range" min="400" max="3500" step="50" value="1200" />
        <span id="silenceMsVal" class="small-note">1200</span>
      </div>
      <div class="control">
        <label><input id="bargeInToggle" type="checkbox" checked /> Allow interrupt</label>
      </div>
    </div>

    <input id="textInput" type="text" placeholder="...or type your question here" />
    <div id="status">Status: Idle</div>
    <div id="originHint" class="small-note"></div>
  </div>

  <div class="panel">
    <h2>Conversation</h2>
    <div id="log"></div>
    <div id="latency" class="small-note"></div>
  </div>

  <script>
    const listenBtn = document.getElementById('listenBtn');
    const stopBtn = document.getElementById('stopBtn');
    const sendBtn = document.getElementById('sendBtn');
    const statusEl = document.getElementById('status');
    const logEl = document.getElementById('log');
    const textInput = document.getElementById('textInput');
    const serverUrlInput = document.getElementById('serverUrl');
    const latencyEl = document.getElementById('latency');
    const sessionInfoEl = document.getElementById('sessionInfo');
    const originHintEl = document.getElementById('originHint');

    const alwaysOnToggle = document.getElementById('alwaysOnToggle');
    const bargeInToggle = document.getElementById('bargeInToggle');
    const vadThreshold = document.getElementById('vadThreshold');
    const vadThresholdVal = document.getElementById('vadThresholdVal');
    const silenceMs = document.getElementById('silenceMs');
    const silenceMsVal = document.getElementById('silenceMsVal');

    // âœ… DEFAULT URL: EXACTLY WHAT YOU GAVE (http + :5000)
    const DEFAULT_SERVER_URL =
      window.VOICE_AGENT_URL ||
      localStorage.getItem('voiceAgentServerUrl') ||
      'http://py-app-capture-agent-svc-dev-1-1768200251.us-east-1.elb.amazonaws.com:5000/voice/conversation';

    // âœ… Normalize WITHOUT forcing https
    function normalizeEndpoint(raw) {
      const trimmed = (raw || '').trim();
      if (!trimmed) return DEFAULT_SERVER_URL;

      // Already absolute
      if (/^https?:\/\//i.test(trimmed)) return trimmed;

      // Looks like a hostname? default to http:// (NOT https://)
      if (trimmed.includes('.') && !trimmed.startsWith('/')) {
        return `http://${trimmed}`;
      }

      // Otherwise treat as relative path
      return trimmed;
    }

    serverUrlInput.value = normalizeEndpoint(DEFAULT_SERVER_URL);

    serverUrlInput.addEventListener('change', () => {
      const normalized = normalizeEndpoint(serverUrlInput.value);
      serverUrlInput.value = normalized;
      try { localStorage.setItem('voiceAgentServerUrl', normalized); } catch (err) {}
    });

    vadThresholdVal.textContent = Number(vadThreshold.value).toFixed(3);
    silenceMsVal.textContent = String(silenceMs.value);

    vadThreshold.addEventListener('input', () => {
      vadThresholdVal.textContent = Number(vadThreshold.value).toFixed(3);
    });
    silenceMs.addEventListener('input', () => {
      silenceMsVal.textContent = String(silenceMs.value);
    });

    function loadOrInitUserId() {
      const storageKey = 'voiceAgentUserId';
      const stored = localStorage.getItem(storageKey);
      if (stored) return stored;
      const token = crypto.randomUUID().split('-')[0];
      const generated = `VOICETEST-${token}`;
      localStorage.setItem(storageKey, generated);
      return generated;
    }

    const userId = loadOrInitUserId();
    const threadId = crypto.randomUUID();
    const rooftopId = '6';

    sessionInfoEl.textContent = `Session ID: ${userId}`;
    let currentTurnMetrics = { preambleLatency: null, finalLatency: null };

    function updateStatus(text) { statusEl.textContent = text; }
    function resetTurnMetrics() {
      currentTurnMetrics = { preambleLatency: null, finalLatency: null };
      latencyEl.textContent = '';
    }
    function applyTurnMetrics() {
      const parts = [];
      if (typeof currentTurnMetrics.preambleLatency === 'number') {
        parts.push(`Preamble: ${Math.round(currentTurnMetrics.preambleLatency)} ms`);
      }
      if (typeof currentTurnMetrics.finalLatency === 'number') {
        parts.push(`Full response: ${Math.round(currentTurnMetrics.finalLatency)} ms`);
      }
      latencyEl.textContent = parts.join(' â€¢ ');
    }

    function escapeHtml(text) {
      const div = document.createElement('div');
      div.textContent = text;
      return div.innerHTML;
    }

    function appendMessage(role, text, label = null) {
      const item = document.createElement('div');
      item.className = `message ${role}`;
      const title = label ? `<strong>${label}:</strong> ` : '';
      item.innerHTML = `${title}${escapeHtml(text)}`;
      logEl.appendChild(item);
      logEl.scrollTop = logEl.scrollHeight;
      return item;
    }

    // Mic + VAD + MediaRecorder
    let micAccessGranted = false;
    let micStream = null;
    let mediaRecorder = null;
    let recordingActive = false;
    let recordedChunks = [];
    let lastRecorderMime = 'audio/webm';

    // VAD
    let audioContext = null;
    let analyser = null;
    let analyserData = null;
    let vadLoopRunning = false;
    let silenceStartAt = null;

    // UI state
    let listeningArmed = false;

    // Playback / barge-in
    let audioQueue = Promise.resolve();
    let playbackController = null;
    let audioPlaying = false;

    function pickRecorderMimeType() {
      if (!window.MediaRecorder) return '';
      const candidates = [
        'audio/webm;codecs=opus',
        'audio/ogg;codecs=opus',
        'audio/webm',
        'audio/ogg',
      ];
      for (const c of candidates) {
        if (MediaRecorder.isTypeSupported(c)) return c;
      }
      return '';
    }

    async function ensureMicPipeline() {
      if (micAccessGranted && micStream && analyser && audioContext) return true;

      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        updateStatus('Status: Microphone not supported in this browser.');
        return false;
      }

      try {
        micStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            sampleRate: 16000,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          }
        });

        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaStreamSource(micStream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        source.connect(analyser);
        analyserData = new Uint8Array(analyser.fftSize);

        micAccessGranted = true;
        console.log('[DEBUG] Mic pipeline ready');
        return true;
      } catch (err) {
        console.error('Microphone permission error', err);
        updateStatus('Status: Microphone permission denied');
        micAccessGranted = false;
        micStream = null;
        return false;
      }
    }

    function startRecording() {
      if (!micStream || recordingActive) return;

      const mimeType = pickRecorderMimeType();
      recordedChunks = [];
      lastRecorderMime = mimeType || 'audio/webm';

      try {
        mediaRecorder = mimeType
          ? new MediaRecorder(micStream, { mimeType })
          : new MediaRecorder(micStream);
      } catch (err) {
        console.error('Failed to start recorder', err);
        updateStatus('Status: Recorder failed to start');
        return;
      }

      mediaRecorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) recordedChunks.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        recordingActive = false;
        const blob = new Blob(recordedChunks, { type: lastRecorderMime });
        recordedChunks = [];
        if (blob.size === 0) return;
        await sendAudioToAgent(blob, lastRecorderMime);
      };

      mediaRecorder.start();
      recordingActive = true;
      updateStatus('Status: Recording...');
    }

    function stopRecording() {
      if (mediaRecorder && recordingActive) {
        try { mediaRecorder.stop(); } catch (err) {}
      }
    }

    function computeRmsVolume() {
      if (!analyser || !analyserData) return 0;
      analyser.getByteTimeDomainData(analyserData);
      let sum = 0;
      for (let i = 0; i < analyserData.length; i++) {
        const normalized = (analyserData[i] - 128) / 128;
        sum += normalized * normalized;
      }
      return Math.sqrt(sum / analyserData.length);
    }

    function armListening() {
      listeningArmed = true;
      listenBtn.disabled = true;
      stopBtn.disabled = false;

      if (alwaysOnToggle.checked) {
        updateStatus('Status: Waiting for speech...');
        startVadLoop();
      } else {
        startRecording();
      }
    }

    function disarmListening() {
      listeningArmed = false;
      listenBtn.disabled = false;
      stopBtn.disabled = true;

      stopRecording();
      stopVadLoop();
      updateStatus('Status: Idle');
    }

    function startVadLoop() {
      if (vadLoopRunning) return;
      if (!analyser) return;

      vadLoopRunning = true;
      silenceStartAt = null;

      const loop = () => {
        if (!vadLoopRunning) return;
        if (!listeningArmed || !alwaysOnToggle.checked) {
          vadLoopRunning = false;
          return;
        }

        const threshold = Number(vadThreshold.value);
        const silenceThresholdMs = Number(silenceMs.value);
        const volume = computeRmsVolume();

        // âœ… interrupt (barge-in)
        if (audioPlaying && bargeInToggle.checked && volume > threshold) {
          stopCurrentPlayback();
        }

        if (volume > threshold) {
          silenceStartAt = null;
          if (!recordingActive) startRecording();
        } else {
          if (recordingActive) {
            if (silenceStartAt === null) {
              silenceStartAt = performance.now();
            } else if (performance.now() - silenceStartAt > silenceThresholdMs) {
              silenceStartAt = null;
              stopRecording();
            }
          }
        }

        requestAnimationFrame(loop);
      };

      requestAnimationFrame(loop);
    }

    function stopVadLoop() {
      vadLoopRunning = false;
      silenceStartAt = null;
    }

    // Playback
    function playAudioSegment(segment, delayFirstChunk = false) {
      if (!segment) return;
      const chunkList = Array.isArray(segment.chunks) && segment.chunks.length
        ? [...segment.chunks]
        : (segment.base64 ? [segment.base64] : []);
      if (!chunkList.length) return;

      audioQueue = audioQueue.then(() => new Promise((resolve) => {
        audioPlaying = true;

        const playNext = (index) => {
          if (index >= chunkList.length) {
            audioPlaying = false;
            playbackController = null;
            resolve();
            return;
          }

          try {
            const binary = atob(chunkList[index]);
            const buffer = new Uint8Array(binary.length);
            for (let i = 0; i < binary.length; i++) buffer[i] = binary.charCodeAt(i);

            const blob = new Blob([buffer], { type: `audio/${segment.format || 'mp3'}` });
            const url = URL.createObjectURL(blob);
            const audio = new Audio(url);

            let cancelled = false;
            playbackController = {
              abort() {
                if (cancelled) return;
                cancelled = true;
                try { audio.pause(); } catch (err) {}
                URL.revokeObjectURL(url);
                audioPlaying = false;
                playbackController = null;
                resolve();
              }
            };

            const startPlayback = () =>
              audio.play().catch(() => {
                URL.revokeObjectURL(url);
                playbackController = null;
                playNext(index + 1);
              });

            audio.onended = () => {
              URL.revokeObjectURL(url);
              if (cancelled) return;
              playNext(index + 1);
            };
            audio.onerror = () => {
              URL.revokeObjectURL(url);
              if (cancelled) return;
              playNext(index + 1);
            };

            if (delayFirstChunk && index === 0) setTimeout(startPlayback, 350);
            else startPlayback();
          } catch (err) {
            playNext(index + 1);
          }
        };

        playNext(0);
      }));
    }

    function stopCurrentPlayback() {
      if (playbackController) {
        const controller = playbackController;
        playbackController = null;
        controller.abort();
      } else {
        audioPlaying = false;
      }
    }

    // Networking
    async function sendToAgent(message) {
      const endpoint = normalizeEndpoint(serverUrlInput.value);
      serverUrlInput.value = endpoint;
      try { localStorage.setItem('voiceAgentServerUrl', endpoint); } catch (err) {}

      const requestBody = { user_id: userId, thread_id: threadId, rooftop_id: rooftopId, message };

      try {
        resetTurnMetrics();
        updateStatus('Status: Contacting voice agent...');

        const response = await fetch(endpoint, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(requestBody),
        });

        if (!response.ok) {
          const text = await response.text();
          throw new Error(`HTTP ${response.status}: ${text}`);
        }

        const payload = await response.json();
        handleNonStreamingResponse(payload);
        updateStatus(listeningArmed ? 'Status: Waiting for speech...' : 'Status: Ready');
      } catch (err) {
        statusEl.textContent = `Status: Error - ${err.message}`;
        appendMessage('agent', 'I hit a snag reaching the service. Try again in a moment.');
      }
    }

    async function sendAudioToAgent(blob, mimeType) {
      const endpoint = normalizeEndpoint(serverUrlInput.value);
      serverUrlInput.value = endpoint;
      try { localStorage.setItem('voiceAgentServerUrl', endpoint); } catch (err) {}

      const formData = new FormData();
      formData.append('user_id', userId);
      formData.append('thread_id', threadId);
      formData.append('rooftop_id', rooftopId);

      const ext = (mimeType && mimeType.includes('ogg')) ? 'ogg' : 'webm';
      formData.append('audio', blob, `audio.${ext}`);

      try {
        resetTurnMetrics();
        updateStatus('Status: Contacting voice agent...');

        const response = await fetch(endpoint, { method: 'POST', body: formData });

        if (!response.ok) {
          const text = await response.text();
          throw new Error(`HTTP ${response.status}: ${text}`);
        }

        const payload = await response.json();
        if (payload && payload.transcript) appendMessage('user', payload.transcript);

        handleNonStreamingResponse(payload);
        updateStatus(listeningArmed ? 'Status: Waiting for speech...' : 'Status: Ready');
      } catch (err) {
        statusEl.textContent = `Status: Error - ${err.message}`;
        appendMessage('agent', 'I hit a snag reaching the service. Try again in a moment.');
      }
    }

    function handleNonStreamingResponse(payload) {
      if (!payload || typeof payload !== 'object') return;

      const response = payload.response || {};
      const text =
        response.spokenMessage ||
        response.suffixMessage ||
        response.prefixMessage ||
        response.followUpMessage ||
        '';

      if (text) appendMessage('agent', text, 'Response');
      else appendMessage('agent', 'No response text returned.', 'Response');

      const audioSegments = Array.isArray(payload.audio_segments) ? payload.audio_segments : [];
      audioSegments.forEach((segment, idx) => {
        if (segment.audio && segment.audio.b64) {
          playAudioSegment({ format: segment.audio.format || 'wav', chunks: [segment.audio.b64] }, idx === 0);
        }
      });
    }

    // UI wiring
    stopBtn.disabled = true;

    listenBtn.addEventListener('click', async () => {
      if (!window.MediaRecorder) {
        listenBtn.disabled = true;
        stopBtn.disabled = true;
        updateStatus('Status: MediaRecorder not supported in this browser.');
        return;
      }
      updateStatus('Status: Initializing microphone...');
      const ready = await ensureMicPipeline();
      if (!ready) {
        listenBtn.disabled = false;
        stopBtn.disabled = true;
        return;
      }
      armListening();
    });

    stopBtn.addEventListener('click', () => {
      disarmListening();
      stopCurrentPlayback();
    });

    sendBtn.addEventListener('click', () => {
      const message = textInput.value.trim();
      if (!message) return;
      appendMessage('user', message);
      sendToAgent(message);
      textInput.value = '';
    });

    textInput.addEventListener('keydown', (event) => {
      if (event.key === 'Enter') sendBtn.click();
    });

    alwaysOnToggle.addEventListener('change', () => {
      if (!listeningArmed) return;
      stopRecording();
      stopVadLoop();
      if (alwaysOnToggle.checked) {
        updateStatus('Status: Waiting for speech...');
        startVadLoop();
      } else {
        startRecording();
      }
    });

    // Hint: HTTPS page -> HTTP backend may be blocked by browser (mixed content).
    (async () => {
      if (window.location.protocol === 'https:') {
        originHintEl.innerHTML =
          '<span class="warn">Note:</span> This page is HTTPS. Browsers may block HTTP calls to the voice service. Use http://localhost or open this file locally (file://) for the demo.';
      }
    })();
  </script>
</body>
</html>
